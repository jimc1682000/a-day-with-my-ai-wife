# 我與老婆 (Claude Code) 的一天 — 講稿

> Human-in-the-Loop (HITL) AI 協作模式實戰案例分析

---

## 講稿使用說明

本講稿使用三層標記系統：

| 標記 | 含義 | 10 分鐘 | 20 分鐘 | 30 分鐘 |
|:----:|------|:-------:|:-------:|:-------:|
| `▰▰▰` | 骨幹 — 刪掉就散架 | ✓ | ✓ | ✓ |
| `▰▰▱` | 血肉 — 讓故事完整 | - | ✓ | ✓ |
| `▰▱▱` | 肌理 — 增加深度與反思 | - | - | ✓ |

---

## 一、開場（2 分鐘）`▰▰▰`

### 講稿

大家好，今天要分享的主題是「我與老婆的一天」。

先說明一下，這裡的「老婆」是指 Claude Code，一個 AI 程式助手。

**為什麼叫老婆？**

因為跟 Claude Code 協作，真的很像跟老婆相處：

- 要好好溝通，不能只丟一句話
- 她很聰明，但有時會搞錯你的意思
- 需要適時提醒和糾正
- 給對方向後，執行力超強
- 同樣的話問兩次，可能得到不同答案
- 聊太久她會忘記前面說過什麼

這不是在開玩笑，這是 AI 的真實特性。

**正經版說法**：這是一份 Human-in-the-Loop 協作模式的實戰案例分析。

### 講者備註

- 可以用輕鬆的語氣開場，但要讓聽眾知道這是認真的技術分享
- 如果聽眾是比較嚴肅的場合，可以先講「正經版說法」，再帶出「老婆」的比喻

---

## 二、背景：那天發生什麼事（3 分鐘）`▰▰▰`

### 講稿

故事發生在某個平凡的早上 11 點。

我們的 Gitea（Git 代管服務）被 OOM Killer 砍了。不只 Gitea 被砍，連帶 supervisord 也被 systemctl 終止。

SSH 連不上，服務中斷，我心跳開始加速。

這時候我打開 Claude Code，說：「老婆，我們的 git-server 被 OOM killed 了，幫我查一下原因。」

她說：「好，讓我 SSH 連進去...」

然後等了很久。

「連不上，SSH timeout 了。建議透過 AWS Console 重啟。」

**協作心法 #1：緊急狀況時先穩住**

AI 很想幫忙，但機器連不上也沒辦法。這時候人類要先處理基礎設施問題。

最厲害的 AI 給的建議，有時候跟 IT 部門的萬年 SOP 一樣：「Have you tried turning it off and on again?」

### 講者備註

- 這段要讓聽眾感受到「真實感」
- 強調這不是虛構案例，是真實發生的事件

---

## 三、AI 協作模式光譜（5 分鐘）`▰▰▰`

### 講稿

在進入實戰案例之前，我想先介紹一個概念：**AI 協作模式光譜**。

從「人類控制」到「AI 自主」，大概可以分成這幾個等級：

| 等級 | 名稱 | 運作方式 | 範例 |
|------|------|----------|------|
| 0 | 已知用火 | 人類全包，不用 AI | Vim、純手寫 |
| 1 | Copilot | AI 建議，人類採納 | GitHub Copilot 自動補全 |
| 2 | **HITL** | 人類決策 → AI 執行 → 人類審核 | Claude Code、Cursor |
| 3 | Delegator | 人類定義範圍，AI 自行完成 | 「幫我寫個腳本」 |
| 4 | Supervisor | AI 執行，異常時通知人類 | CI/CD 自動部署 |
| 5 | Autonomous | 給目標，AI 完全自主 | AutoGPT、Devin |

**我們這次用的是 Level 2：Human-in-the-Loop**

- 人類掌握決策權
- AI 負責執行
- 每個重要動作需要人類確認
- 可以即時糾正

為什麼不用更高級的自主模式？因為：

1. 這是生產環境，出錯代價很高
2. 涉及 SSH、AWS、Terraform，需要人類判斷
3. AI 會犯錯，需要即時糾正

### 講者備註

- 這張表可以做成投影片
- 強調「沒有最好的模式，只有最適合的模式」

---

## 三之二、HITL × 護欄：雙軸治理（3 分鐘）`▰▰▱`

### 講稿

剛剛講的是「人要介入多少」，但還有另一個維度：**護欄**。

我用一首歌來解釋：

> 「我很好騙 對愛太渴望變成死穴 / 所有防備 全都防不了孤單侵略」

這首歌描述的是什麼？**純靠意志力的脆弱狀態**。

渴望最強烈的時候，通常也是意志力最弱的時候。這時候所有防備都會失效。

用這首歌形容我跟 AI 的協作關係貼切嗎？**不貼切**。

為什麼？因為我們不是純靠意志力在撐——我們有**護欄**。

**人治 vs 法治**

| 治理類型 | 定義 | 弱點 |
|----------|------|------|
| **人治** | 依賴人的意志力與判斷 | 意志力最弱時最危險；每個人標準不一 |
| **法治** | 依賴規則與機制 | 需要事先設計好 |

HITL 是「人治」，護欄是「法治」。

關鍵是：**不是二選一，而是用法治來支撐人治**。

**2×2 矩陣**

```
              ← 弱法治    強法治 →

  高人治 ↑      🔴          🟢

  低人治 ↓      ⚫          🔵
```

- 🟢 **穩健**：高人治 + 強法治 = 雙重保護
- 🔴 **脆弱**：高人治 + 弱法治 = 純靠意志力，累了就漏
- ⚫ **危險**：低人治 + 弱法治 = 裸奔
- 🔵 **自治**：低人治 + 強法治 = 成熟目標

**遷移路徑**

大部分人起步時是 🔴 脆弱象限：很認真在審，但沒有護欄。

目標是往 🔵 自治移動：把經驗固化成護欄規則，人只處理例外。

> 「不是變得不會受騙，而是把防禦從意志力改成結構。」

### 講者備註

- 這段用「我很好騙」開場可以引起共鳴
- 2×2 矩陣可以做成投影片
- 如果時間不夠，可以跳過這段，直接進案例

---

## 四、實戰案例：七次糾正（10 分鐘）

### 4.1 案例一：不是所有建議都適合（2 分鐘）`▰▰▰`

#### 講稿

OOM 問題出現後，老婆第一個建議是：「加 2GB Swap」。

我說：「等等，AWS EC2 不是不建議開 swap 嗎？」

她說：「對，EBS 的 I/O 延遲太高，swap 效果不好。那我們用其他方法。」

**協作心法 #2：質疑不適合環境的建議**

AI 會給通用建議，但人類要判斷是否適合自己的環境。

這就像問老婆「晚餐吃什麼」，她可能說「吃火鍋」，但如果你正在減肥，你得自己判斷這建議適不適合。

#### 講者備註

- 這是最容易理解的案例，適合所有聽眾

---

### 4.2 案例二：子進程繼承問題（3 分鐘）`▰▰▰`

#### 講稿

接下來我們要保護 supervisord 不被 OOM Killer 砍掉。

老婆建議：「設定 OOMScoreAdjust=-500 保護 supervisord」

我說好，部署下去。

結果我檢查 oom_score_adj 的時候發現：

```
supervisord: -500
gitea:       -500  ← 等等，為什麼？
caddy:       -500
gitea-runner:-500
```

我問：「為什麼 gitea 也是 -500？我們只改了 supervisord 啊？」

老婆說：「啊... 這是 Linux 的 fork 機制。子進程會繼承父進程的 oom_score_adj。」

我們原本想保護 supervisord，結果連 OOM 元凶 gitea 也被保護了。

**協作心法 #4：實際部署後要驗證**

理論上可行，不代表實際上可行。部署後一定要確認效果。

#### 講者備註

- 這個案例展示了「AI 也會忽略系統層面的副作用」
- 可以畫一個 fork 繼承的示意圖

---

### 4.3 案例三：換個思路（2 分鐘）`▰▰▱`

#### 講稿

發現繼承問題後，老婆建議：「改成 -900 保護 supervisord，然後用 wrapper 設定各子進程的分數。」

我跟主管討論後說：「等等，這樣不太對。如果我們把 application 的 OOM score 壓低，可能會讓 kernel 去砍更底層的服務，像 sshd，那會更慘。」

老婆說：「對，這樣確實風險更高。」

我說：「那我們換個思路。與其保護 supervisord，不如**拉高 gitea**，讓它更容易被砍？」

老婆：「對！這是更合理的做法。」

| 做法 | 問題 |
|------|------|
| 保護 supervisord (-900) | 可能害死其他服務 |
| 拉高 gitea (+200) | ✅ 無副作用 |

**協作心法 #5：考慮副作用和合理性**

技術上可行，不代表是好做法。

#### 講者備註

- 這個案例展示「人類的系統思維」比 AI 更全面

---

### 4.4 案例四：先查既有模式（2 分鐘）`▰▰▱`

#### 講稿

下午要做備份，我請老婆用 Terraform 加一個 40GB EBS。

Terraform plan 跑下去，顯示：**instance must be replaced**

我嚇出一身冷汗：「等等！先確認我們有沒有類似的用法？」

老婆翻了一下現有的 Terraform，說：「找到了！應該用 `ebs_volumes:` 才不會觸發 instance 替換。」

```yaml
# ❌ 錯誤：會替換 instance
ebs:
  - volume_type: gp3

# ✅ 正確：獨立的 volume attachment
ebs_volumes:
  - resource_name: backup
```

**協作心法 #7：先查既有模式再動手**

「我們之前怎麼做的？」這句話可以避免很多災難。

#### 講者備註

- 這是差點出大事的案例，可以講得緊張一點

---

### 4.5 案例五：工具能力確認（1 分鐘）`▰▱▱`

#### 講稿

備份要上傳到 S3，我說：「用 tag-based lifecycle rule，這是我們的慣例。」

老婆開始設定，我突然問：「等等，s5cmd 有支援設定 tag 嗎？」

老婆查了一下：「...不支援。」

那就只能改用 prefix-based lifecycle。

**協作心法 #10：確認工具能力**

即使是人類指定的做法，也要確認工具是否支援。

---

### 4.6 案例六：排程衝突（1 分鐘）`▰▰▱`

#### 講稿

設定備份 cron 時，老婆說：「0 18 * * * — 備份」

我看了一眼：「等等，gitea 的 health check 也是 18:00 跑，這不會撞車嗎？」

老婆：「對欸！我改成 20:00。」

**協作心法 #13：檢查排程衝突**

AI 專注在單一任務，跨系統的衝突需要人類把關。

---

### 4.7 案例七：安全把關（1 分鐘）`▰▰▱`

#### 講稿

最後整理 Ansible 時，我發現 group_vars 裡面有個檔案：

「等等，這個檔案有 token，應該從一開始就要移掉。」

老婆說：「好，我用 filter-branch 重寫歷史。」

**協作心法 #14：安全意識人類把關**

AI 不會主動檢查是否 commit 了敏感資訊。這是人類的責任。

### 講者備註

- 安全議題很重要，即使時間短也要提

---

## 五、讓 AI 操作 CLI 真的安全嗎？（5 分鐘）`▰▰▱`

### 講稿

很多人會問：「讓 AI 操作 CLI，不危險嗎？」

我想先介紹一個心理學概念：**控制的錯覺**。

心理學家 Ellen Langer 發現：當人們可以「自己選彩票號碼」時，會對中獎更有信心——即使機率完全一樣。

很多人覺得「自己打指令比較安全」，其實是同樣的心理：

| 控制的錯覺 | 真正的控制 |
|------------|------------|
| 我按了 Enter | 我理解這個指令在做什麼 |
| 我選了這個選項 | 我能驗證結果是否正確 |
| 我親手打的字 | 我能在出錯時即時修正 |

**安全性來自「指令正確 + 有審核機制」，不是「誰按 Enter」。**

### 58 次中斷的數據

這一天的協作中，我總共中斷了 AI 58 次。

| 中斷原因 | 佔比 |
|----------|------|
| 想先看計畫 | 30% |
| 自己手動做 | 25% |
| 補充環境脈絡 | 20% |
| 驗證先行 | 15% |
| 遵循團隊慣例 | 10% |

關鍵發現：

- **0 次**是因為 AI 要執行「危險操作」
- **0 次**是因為 AI「完全搞錯方向」
- **0 次**造成「實際損害」需要回復
- **100%** 是人類「主動介入」維持掌控權

**風險的真正來源不是「AI 會亂來」，而是「人類是否保持關注」。**

### 講者備註

- 這段是整個分享的核心論點之一
- 58 次中斷的數據來自單一案例，要誠實說明

---

## 六、把 AI 當老婆？小心這些誤區（3 分鐘）`▰▱▱`

### 講稿

「老婆」的比喻很好懂，但有些地方會誤導：

| 老婆的特質 | AI 的實際情況 | 風險 |
|------------|---------------|------|
| 記得你 | 每次 session 都是新的 | 省略重要脈絡 |
| 會成長 | 不會從互動中學習 | 期待落空 |
| 有責任感 | 出事 AI 不負責 | 推卸責任 |
| 在乎你 | 只是在執行指令 | 情感依賴 |
| 會反對你 | 傾向順從 | 缺乏制衡 |

**正確的用法**：

- 相處方式學老婆：耐心溝通、適時糾正
- 但別忘了：她說的要驗證、出事你要扛、她不記得上次聊什麼

### 講者備註

- 這段是反思，避免聽眾過度擬人化 AI

---

## 七、結論（2 分鐘）`▰▰▰`

### 講稿

回顧這一天：

- ⏱️ 7.5 小時的協作
- 💻 200+ 行程式碼
- 💡 14 個協作心法
- 🔧 7 次糾正

如果 AI 是人類，這大概是會讓她想離職的一天。但她不是，所以她只是說：「好的，我來修改。」

**核心結論**：

> AI 是很強的執行者，但需要人類當導演。

Human-in-the-Loop 不是因為我們不信任 AI，而是因為：

1. AI 會犯錯，需要即時糾正
2. AI 不知道你的環境和慣例
3. AI 專注單一任務，缺乏系統視角
4. 出事的時候，是人類要負責

目前的 AI 技術，HITL 仍是生產環境最可靠的選擇。

### 講者備註

- 結尾要有力，重複核心訊息
- 可以用「明天見，老婆」作為幽默收尾

---

## 八、Q&A 預備題

### Q1：你用的是什麼工具？

Claude Code，Anthropic 官方的 CLI 工具。類似的工具還有 Cursor、Aider、GitHub Copilot Chat 等。

### Q2：這樣協作會不會比自己做更慢？

短期可能差不多，但長期來看：
- AI 可以處理重複性的 boilerplate
- AI 可以快速查資料
- 人類可以專注在決策和審核

### Q3：AI 會不會取代工程師？

以目前的技術來看，AI 更像是「超級實習生」：
- 能力很強、動作很快
- 但需要指導和審核
- 不能完全放手讓他做

### Q4：怎麼開始嘗試這種協作模式？

建議從低風險的任務開始：
- 寫測試
- 寫文件
- 重構小範圍的程式碼

熟悉之後再嘗試更複雜的任務。

---

## 時間控制檢查表

### 10 分鐘版 — 只講 `▰▰▰`

- [ ] 一、開場（2 分鐘）
- [ ] 二、背景（2 分鐘，簡述）
- [ ] 三、協作模式光譜（3 分鐘，聚焦 HITL）
- [ ] 四、案例精選（2 分鐘）：4.1 不適合環境 + 4.2 子進程繼承
- [ ] 七、結論（1 分鐘）

### 20 分鐘版 — 講 `▰▰▰` + `▰▰▱`

- [ ] 一、開場（2 分鐘）
- [ ] 二、背景（3 分鐘）
- [ ] 三、協作模式光譜（3 分鐘）
- [ ] 三之二、HITL × 護欄（3 分鐘）
- [ ] 四、案例（6 分鐘）：4.1 ~ 4.4 + 4.6 + 4.7
- [ ] 五、CLI 安全性（2 分鐘，精簡）
- [ ] 七、結論（1 分鐘）

### 30 分鐘版 — 全部內容

- [ ] 一～七全部照講稿
- [ ] 包含三之二、HITL × 護欄
- [ ] 包含六、誤區反思
- [ ] 包含 4.5 工具能力確認

---

## 投影片建議大綱

| # | 標題 | 層級 |
|---|------|:----:|
| 1 | 標題頁 | `▰▰▰` |
| 2 | 為什麼叫老婆？ | `▰▰▰` |
| 3 | 背景：Gitea OOM | `▰▰▰` |
| 4 | AI 協作模式光譜 | `▰▰▰` |
| 5 | HITL × 護欄：人治 vs 法治 | `▰▰▱` |
| 6 | HITL × 護欄：2×2 矩陣 | `▰▰▱` |
| 7 | 案例一：不適合環境的建議 | `▰▰▰` |
| 8 | 案例二：子進程繼承 | `▰▰▰` |
| 9 | 案例三：換個思路 | `▰▰▱` |
| 10 | 案例四：先查既有模式 | `▰▰▱` |
| 11 | 案例五：工具能力確認 | `▰▱▱` |
| 12 | 案例六：排程衝突 | `▰▰▱` |
| 13 | 案例七：安全把關 | `▰▰▱` |
| 14 | 控制的錯覺 | `▰▰▱` |
| 15 | 58 次中斷的數據 | `▰▰▱` |
| 16 | 老婆比喻的誤區 | `▰▱▱` |
| 17 | 結論：AI 是執行者，人類是導演 | `▰▰▰` |
| 18 | Q&A | `▰▰▰` |

---

*講稿版本：v1.0*
*最後更新：2026-01*
