# 讓 AI 操作 CLI 真的安全嗎？

> 來自 12 小時真實協作的數據與反思

> ⚠️ **數據說明**：本分析基於單一協作 session（約 12 小時），統計結果可能受個人工作風格影響，僅供參考。歡迎其他工程師分享自己的協作數據。

---

## 一、控制的錯覺

> 「我自己打指令比較安全」——這可能是一種**控制的錯覺**。

### 什麼是控制的錯覺？

心理學家 Ellen Langer 提出：人們傾向於**高估自己控制場面的能力**。

經典實驗：當人們可以「自己選彩票號碼」時，會對中獎更有信心——即使中獎機率根本一樣。

### 但 CLI 不是彩票

| 情境 | 彩票 | CLI |
|------|------|-----|
| 結果可控？ | ❌ 完全隨機 | ✅ 指令決定結果 |
| 你能影響結果？ | ❌ 選號不影響 | ⚠️ **這裡有陷阱** |

CLI 結果**確實是確定性的**——正確的指令產生正確的結果。

但陷阱在於：
- ✅ 「指令內容」決定結果
- ❌ 「誰打指令」決定結果 ← **這才是錯覺所在**

### 真正的控制來自哪裡？

| 控制的錯覺 | 真正的控制 |
|------------|------------|
| 我按了 Enter | 我理解這個指令在做什麼 |
| 我選了這個選項 | 我能驗證結果是否正確 |
| 我親手打的字 | 我能在出錯時即時修正 |

**安全性來自「指令正確 + 有審核機制」，不是來自「誰按 Enter」。**

---

## 二、58 次中斷的真相

這份分析來自一整天（約 12 小時）的人類-AI 協作，處理 Gitea OOM 問題，涉及 SSH、AWS CLI、Terraform、Ansible 等操作。

### 中斷統計

| 操作類型 | 次數 | 佔比 |
|----------|------|------|
| SSH 遠端操作 | 20 | 34% |
| 搜尋/讀取 | 10 | 17% |
| 編輯檔案 | 6 | 10% |
| Terraform | 4 | 7% |
| AWS CLI | 3 | 5% |
| 其他 | 16 | 27% |

### 中斷原因

| 原因 | 佔比 | 例子 |
|------|------|------|
| **想先看計畫** | 30% | 「先跟我說你打算怎麼修 ini...」 |
| **自己手動做** | 25% | 「我手動 apply 完了」 |
| **補充環境脈絡** | 20% | 「我們的 supervisord 是用 ansible 裝的」 |
| **驗證先行** | 15% | 「先確認 choom 有沒有 permission 問題」 |
| **遵循團隊慣例** | 10% | 「naming 要 follow 我們的 convention」 |

### 關鍵發現

```
58 次中斷中：

⭕ 0 次是因為 AI 要執行「危險操作」
⭕ 0 次是因為 AI「完全搞錯方向」
⭕ 0 次造成「實際損害」需要回復

✅ 100% 是人類「主動介入」維持掌控權
```

**風險的真正來源不是「AI 會亂來」，而是「人類是否保持關注」。**

---

## 三、漸進式信任：Level 0 到 Level 4

### 信任等級

| Level | AI 可以做什麼 | 你的角色 | 控制來源 | 遊戲類比 🎮 |
|-------|---------------|----------|----------|-------------|
| **0** | 只能查資料、讀檔案 | 你執行所有操作 | 你做所有事 | 自己玩，AI 只能查攻略給你看 |
| **1** | 執行唯讀指令 | 觀察 AI 操作 | 操作無副作用 | 讓 AI 碰搖桿，但只能開背包、看地圖 |
| **2** | 在測試環境寫入 | 驗證結果 | 環境隔離 | 給 AI 測試存檔，搞砸了刪掉重來 |
| **3** | 執行需確認的操作 | 審核每個動作 | 執行前審核 | 讓 AI 玩主存檔，但每個選項都要你選 |
| **4** | 執行，你監督 ⭐ | 監督導正 | 理解 + 可中斷 | 讓 AI 玩，你在旁邊喊「小心後面」 |

> ⭐ **本次協作採用 Level 4**，必要時降級到 Level 3

### 為什麼 Level 4 有效？

```
傳統思維：我要自己打指令 → 我才有控制權
                         ↓
                    控制的錯覺

Level 4：AI 執行 + 我理解 + 我能中斷 + 我能糾正 → 真正的控制
```

**Level 4 的關鍵**：AI 不會忽略你的指令、不會因為你一直喊「往左」而不開心——他只是可能聽不懂，所以**溝通**很重要。

---

## 四、實戰指南

### 開始前

- 告知 AI 你的環境（AWS region、團隊慣例、既有架構）
- 說明哪些操作需要先問過你
- 提供相關檔案路徑讓 AI 參考既有模式

### 過程中

- 要求 AI 先說計畫再執行
- 對不確定的操作要求 dry-run
- 善用 **ESC 鍵**中斷，不需要等 AI 做完
- 發現方向偏了就立即糾正

### 信任公式

```
可控風險 = 透明度 × 可中斷性 × 人類判斷力
```

| 要素 | 說明 |
|------|------|
| 透明度 | AI 會說它要做什麼 |
| 可中斷性 | ESC 隨時可以停 |
| 人類判斷力 | 你要有領域知識來判斷對錯 |

**三者缺一，風險就會放大。**

### 不建議的做法

| 做法 | 風險 |
|------|------|
| 完全不用 AI 操作 CLI | 錯失效率提升 |
| 完全信任不看過程 | 缺乏糾錯機會 |
| 不提供環境脈絡 | AI 容易走偏 |

---

## 結論

> 「不敢讓 AI 操作 CLI」的擔憂是合理的，但解法不是「完全不用」，而是「建立正確的協作模式」。

58 次中斷的數據告訴我們：**當人類保持在 loop 中，AI 操作 CLI 不但安全，而且比疲勞的人類手動操作更可靠。**

很多人堅持「自己打指令比較安全」，就像堅持「自己選彩票號碼比較會中」——把「參與感」誤認為「控制力」。

**真正的風險管理**不是「誰按 Enter」，而是：
1. 你知道這個操作在做什麼嗎？
2. 你能確認結果是否正確嗎？
3. 出錯時你能即時介入嗎？

這三點，不管是你打指令還是 AI 打指令，都一樣重要。

---

### 延伸閱讀

- [控制的錯覺 - 維基百科](https://zh.wikipedia.org/zh-tw/控制的錯覺)
- [控制的錯覺：你真的掌控全局了嗎？](https://vocus.cc/article/68304574fd89780001cb8f7a)

---

*資料來源：2026-01-13 Gitea OOM 問題處理 session，約 12 小時協作記錄*
